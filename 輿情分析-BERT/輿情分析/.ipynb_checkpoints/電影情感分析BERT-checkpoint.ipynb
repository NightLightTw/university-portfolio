{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0cc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922642fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from transformers import BertTokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\" #英文pretrain(不區分大小寫)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d010379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size 30522\n",
      "token               index          \n",
      "-------------------------\n",
      "##lous              15534\n",
      "freshly             20229\n",
      "careful              6176\n",
      "refers               5218\n",
      "devotees            22707\n",
      "towers               7626\n",
      "paranoia            27890\n",
      "##tative            27453\n",
      "likeness            28275\n",
      "cops                10558\n"
     ]
    }
   ],
   "source": [
    "# get pre-train tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"dict size\", len(vocab))\n",
    "\n",
    "# see some token and index mapping\n",
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids): #隨便看幾個字\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6f4679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 24000\n",
      "valset size: 1000\n",
      "testset size:  25000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset,random_split\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = TAG_RE.sub('', sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def readIMDB(path, seg):\n",
    "    classes = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in classes:\n",
    "        files = os.listdir(os.path.join(path, seg, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([preprocess_text(review), 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([preprocess_text(review), 0])\n",
    "    return data\n",
    "\n",
    "label_map = {0: 'neg', 1: 'pos'}\n",
    "\n",
    "#create Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  \n",
    "        self.mode = mode\n",
    "        self.df = readIMDB('aclImdb',mode) #its list [['text1',label],['text2',label],...]\n",
    "        self.len = len(self.df)\n",
    "        self.maxlen = 300 #限制文章長度(若你記憶體夠多也可以不限)\n",
    "        self.tokenizer = tokenizer  # we will use BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        origin_text = self.df[idx][0]\n",
    "        if self.mode == \"test\":\n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            #label_tensor = None #in our case, we have label\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        else:     \n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        \n",
    "        # 建立第一個句子的 BERT tokens\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a[:self.maxlen] + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        if text_b is not None:\n",
    "            tokens_b = self.tokenizer.tokenize(text_b)\n",
    "            word_pieces += tokens_b + [\"[SEP]\"]\n",
    "            len_b = len(word_pieces) - len_a\n",
    "               \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        if text_b is None:\n",
    "            segments_tensor = torch.tensor([1] * len_a,dtype=torch.long)\n",
    "        elif text_b is not None:\n",
    "            segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor, origin_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# initialize Dataset\n",
    "trainset = MyDataset(\"train\", tokenizer=tokenizer)\n",
    "testset = MyDataset(\"test\", tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#split val from trainset\n",
    "val_size = int(trainset.__len__()*0.04) #比對LSTM 切出1000筆當validation\n",
    "trainset, valset = random_split(trainset,[trainset.__len__()-val_size,val_size])\n",
    "print('trainset size:' ,trainset.__len__())\n",
    "print('valset size:',valset.__len__())\n",
    "print('testset size: ',testset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb35e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:\n",
      " ['[CLS]', 'i', 'remember', 'when', 'this', 'was', 'in', 'theaters', 'reviews', 'said', 'it', 'was', 'horrible', 'well', 'didn', 'think', 'it', 'was', 'that', 'bad', 'it', 'was', 'amusing', 'and', 'had', 'lot', 'of', 'tongue', 'in', 'cheek', 'humor', 'concerning', 'families', 'around', 'holiday', 'time', 'ben', 'af', '##fle', '##ck', 'is', 'rich', 'guy', 'who', 'needs', 'to', 'find', 'family', 'for', 'christmas', 'to', 'please', 'his', 'girlfriend', 'he', 'goes', 'to', 'visit', 'the', 'house', 'he', 'grew', 'up', 'in', 'and', 'strikes', 'deal', 'to', 'rent', 'the', 'family', 'there', 'for', 'christmas', 'really', 'liked', 'the', 'lawyer', 'scene', 'where', 'they', 'sign', 'contract', 'that', 'was', 'funny', 'so', 'he', 'makes', 'silly', 'requests', 'of', 'the', 'family', 'and', 'even', 'writes', 'scripts', 'for', 'them', 'to', 'read', 'of', 'course', 'the', 'family', 'has', 'hot', 'daughter', 'for', 'the', 'love', 'interest', 'and', 'he', 'learns', 'that', 'the', 'holidays', 'aren', 'so', 'bad', 'after', 'all', 'also', 'the', 'whole', 'doo', 'da', '##h', 'act', 'was', 'funny', 'especially', 'when', 'they', 'replaced', 'the', 'first', 'one', 'with', 'black', 'guy', 'and', 'the', 'girlfriends', 'parents', 'didn', 'even', 'say', 'anything', 'about', 'it', 'and', 'the', 'parts', 'where', 'doo', 'da', '##h', 'is', 'hitting', 'on', 'his', 'supposed', 'daughter', 'final', 'verdict', 'thought', 'it', 'worth', 'checking', 'out', 'if', 'you', 'catch', 'it', 'on', 'cable', '[SEP]'] \n",
      "\n",
      "origin_text:\n",
      " I remember when this was in theaters reviews said it was horrible Well didn think it was that bad It was amusing and had lot of tongue in cheek humor concerning families around holiday time Ben Affleck is rich guy who needs to find family for Christmas to please his girlfriend He goes to visit the house he grew up in and strikes deal to rent the family there for Christmas really liked the lawyer scene where they sign contract That was funny So he makes silly requests of the family and even writes scripts for them to read Of course the family has hot daughter for the love interest And he learns that the holidays aren so bad after all Also the whole doo dah act was funny especially when they replaced the first one with black guy and the girlfriends parents didn even say anything about it And the parts where doo dah is hitting on his supposed daughter FINAL VERDICT thought it worth checking out if you catch it on cable  \n",
      "\n",
      "label: neg \n",
      "\n",
      "tokens_tensor:\n",
      " tensor([  101,  1045,  3342,  2043,  2023,  2001,  1999, 12370,  4391,  2056,\n",
      "         2009,  2001,  9202,  2092,  2134,  2228,  2009,  2001,  2008,  2919,\n",
      "         2009,  2001, 19142,  1998,  2018,  2843,  1997,  4416,  1999,  5048,\n",
      "         8562,  7175,  2945,  2105,  6209,  2051,  3841, 21358, 21031,  3600,\n",
      "         2003,  4138,  3124,  2040,  3791,  2000,  2424,  2155,  2005,  4234,\n",
      "         2000,  3531,  2010,  6513,  2002,  3632,  2000,  3942,  1996,  2160,\n",
      "         2002,  3473,  2039,  1999,  1998,  9326,  3066,  2000,  9278,  1996,\n",
      "         2155,  2045,  2005,  4234,  2428,  4669,  1996,  5160,  3496,  2073,\n",
      "         2027,  3696,  3206,  2008,  2001,  6057,  2061,  2002,  3084, 10021,\n",
      "        11186,  1997,  1996,  2155,  1998,  2130,  7009, 14546,  2005,  2068,\n",
      "         2000,  3191,  1997,  2607,  1996,  2155,  2038,  2980,  2684,  2005,\n",
      "         1996,  2293,  3037,  1998,  2002, 10229,  2008,  1996, 11938,  4995,\n",
      "         2061,  2919,  2044,  2035,  2036,  1996,  2878, 20160,  4830,  2232,\n",
      "         2552,  2001,  6057,  2926,  2043,  2027,  2999,  1996,  2034,  2028,\n",
      "         2007,  2304,  3124,  1998,  1996, 27408,  3008,  2134,  2130,  2360,\n",
      "         2505,  2055,  2009,  1998,  1996,  3033,  2073, 20160,  4830,  2232,\n",
      "         2003,  7294,  2006,  2010,  4011,  2684,  2345, 14392,  2245,  2009,\n",
      "         4276,  9361,  2041,  2065,  2017,  4608,  2009,  2006,  5830,   102]) \n",
      "\n",
      "segment tensor:\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# 隨便選一個樣本\n",
    "sample_idx = 10\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor,origin_text = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "\n",
    "print('token:\\n',tokens,'\\n')\n",
    "print('origin_text:\\n',origin_text,'\\n')\n",
    "print('label:',label_map[int(label_tensor.numpy())],'\\n')\n",
    "print('tokens_tensor:\\n',tokens_tensor,'\\n')\n",
    "print('segment tensor:\\n',segments_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd07f50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  2387,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  ...,     0,     0,     0],\n",
      "        [  101, 10166,  2253,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2058, 10052,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  3866,  ...,     0,     0,     0],\n",
      "        [  101,  4516,  2055,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\"\"\"\"\n",
    "create_mini_batch(samples)吃上面定義的mydataset\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "#collate_fn: 如何將多個樣本的資料連成一個batch丟進 model\n",
    "#截長補短後要限制attention只注意非pad 的部分\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 訓練集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad到該batch下最長的長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 batch size 個訓練樣本的 DataLoader\n",
    "# 利用 'collate_fn' 將 list of samples 合併成一個 mini-batch\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "\n",
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "print(tokens_tensors)\n",
    "print(segments_tensors)\n",
    "print(masks_tensors)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7130ee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name      module\n",
      "--------------------\n",
      "bert      embeddings\n",
      "bert      encoder\n",
      "bert      pooler\n",
      "dropout    Dropout(p=0.1, inplace=False)\n",
      "classifier Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "name      module\n",
    "--------------------\"\"\")\n",
    "\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(\"{:10}{}\".format(name,n) )\n",
    "    else:\n",
    "        print(\"{:10} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "[epoch 1] loss: 0.2684, acc: 0.8895, val loss: 0.223508, val acc: 0.912698\n",
      "[epoch 2] loss: 0.1552, acc: 0.9430, val loss: 0.230513, val acc: 0.914683\n",
      "[epoch 3] loss: 0.0873, acc: 0.9716, val loss: 0.240337, val acc: 0.913690\n",
      "[epoch 4] loss: 0.0508, acc: 0.9831, val loss: 0.325541, val acc: 0.911706\n",
      "[epoch 5] loss: 0.0327, acc: 0.9903, val loss: 0.348542, val acc: 0.908730\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    correct = 0\n",
    "    #total = 0\n",
    "    train_loss , val_loss = 0.0 , 0.0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        n += 1\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        # outputs 的順序是 \"(loss), logits, (hidden_states), (attentions)\"\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #get prediction and calulate acc\n",
    "        logits = outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        train_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    #validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valloader:\n",
    "            m += 1\n",
    "            tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "            val_outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "            \n",
    "            logits = val_outputs[1]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            val_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "            val_loss += val_outputs[0].item()\n",
    "\n",
    "    print('[epoch %d] loss: %.4f, acc: %.4f, val loss: %4f, val acc: %4f' %\n",
    "          (epoch+1, train_loss/n, train_acc/n, val_loss/m,  val_acc/m  ))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d087fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "true=[]\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in testloader:\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "        val_outputs = model(input_ids=tokens_tensors, \n",
    "                    token_type_ids=segments_tensors, \n",
    "                    attention_mask=masks_tensors, \n",
    "                    labels=labels)\n",
    "\n",
    "        logits = val_outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        true.extend(labels.cpu().tolist())\n",
    "        predictions.extend(pred.cpu().tolist())\n",
    "\n",
    "\n",
    "c = confusion_matrix(true, predictions)\n",
    "print(c)\n",
    "accuracy_score(predictions,true)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
