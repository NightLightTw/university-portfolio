{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from transformers import BertTokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\" #中文\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size 21128\n",
      "token               index          \n",
      "-------------------------\n",
      "hgih                11674\n",
      "綦                    5201\n",
      "##苁                 18774\n",
      "黏                    7945\n",
      "珑                    4400\n",
      "012                 11496\n",
      "ぬ                     559\n",
      "记                    6381\n",
      "##放                 16180\n",
      "##‧                 13501\n"
     ]
    }
   ],
   "source": [
    "# get pre-train tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"dict size\", len(vocab))\n",
    "\n",
    "# see some token and index mapping\n",
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids): #隨便看幾個字\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 14\n",
      "valset size: 3\n",
      "testset size:  17\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset,random_split\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = TAG_RE.sub('', sen)\n",
    "    # Remove punctuations \n",
    "    sentence = re.sub('[，。,、；.？]', ' ', sentence)\n",
    "    # Removing URL\n",
    "    sentence = re.sub('[a-zA-z]+://[^\\s]*', ' ', sentence)\n",
    "    sentence = re.sub('/((?:https?\\:\\/\\/|www\\.)(?:[-a-z0-9]+\\.)*[-a-z0-9]+.*)/i', ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def readDATA(path, seg):\n",
    "    classes = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in classes:\n",
    "        files = os.listdir(os.path.join(path, seg, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([preprocess_text(review), 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([preprocess_text(review), 0])\n",
    "    return data\n",
    "\n",
    "label_map = {0: 'neg', 1: 'pos'}\n",
    "\n",
    "#create Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  \n",
    "        self.mode = mode\n",
    "        self.df = readDATA('dataset1-100',mode) #its list [['text1',label],['text2',label],...]\n",
    "        self.len = len(self.df)\n",
    "        self.maxlen = 300 #限制文章長度(若你記憶體夠多也可以不限)\n",
    "        self.tokenizer = tokenizer  # we will use BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        origin_text = self.df[idx][0]\n",
    "        if self.mode == \"test\":\n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            #label_tensor = None #in our case, we have label\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        else:     \n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        \n",
    "        # 建立第一個句子的 BERT tokens\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a[:self.maxlen] + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        if text_b is not None:\n",
    "            tokens_b = self.tokenizer.tokenize(text_b)\n",
    "            word_pieces += tokens_b + [\"[SEP]\"]\n",
    "            len_b = len(word_pieces) - len_a\n",
    "               \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        if text_b is None:\n",
    "            segments_tensor = torch.tensor([1] * len_a,dtype=torch.long)\n",
    "        elif text_b is not None:\n",
    "            segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor, origin_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# initialize Dataset\n",
    "trainset = MyDataset(\"train\", tokenizer=tokenizer)\n",
    "testset = MyDataset(\"test\", tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#split val from trainset\n",
    "val_size = int(trainset.__len__()*0.2) #比對LSTM 切出1000筆當validation 0.04\n",
    "trainset, valset = random_split(trainset,[trainset.__len__()-val_size,val_size])\n",
    "print('trainset size:' ,trainset.__len__())\n",
    "print('valset size:',valset.__len__())\n",
    "print('testset size: ',testset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:\n",
      " ['[CLS]', '之', '前', '購', '買', '這', '家', '[UNK]', '[UNK]', '80', '口', '罩', 'cc', '/', 'e5', '##r', '##6', '##g', '##x', '如', '第', '一', '頁', '[UNK]', '表', '格', '顯', '示', '它', '並', '無', '衛', '福', '部', '的', '醫', '材', '編', '號', '法', '規', '上', '來', '說', '不', '算', '醫', '療', '用', '或', '外', '科', '手', '術', '口', '罩', '但', '第', '二', '頁', '[UNK]', '來', '說', '[UNK]', '80', '的', '[UNK]', '[UNK]', '防', '護', '效', '果', '都', '大', '於', '[UNK]', '（', '臺', '灣', '外', '科', '口', '罩', '標', '準', '）', '很', '難', '想', '像', '既', '然', '規', '格', '更', '高', '為', '何', '不', '去', '取', '得', '認', '證', '跟', '醫', '材', '編', '號', '想', '請', '教', '是', '否', '在', '製', '程', '環', '境', '或', '其', '他', '未', '列', '出', '的', '技', '術', '規', '格', '上', '仍', '有', '不', '及', '外', '科', '手', '術', '之', '處', '謝', '謝', '\"', '[SEP]'] \n",
      "\n",
      "origin_text:\n",
      " 之前購買這家MOTEX N 80口罩 cc/e5r6gx 如第一頁PDF表格顯示 它並無衛福部的醫材編號 法規上來說不算醫療用或外科手術口罩 但第二頁PDF來說 N 80的BFE PFE防護效果都大於CNS14755（臺灣外科口罩標準） 很難想像既然規格更高 為何不去取得認證跟醫材編號 想請教是否在製程環境或其他未列出的技術規格上 仍有不及外科手術之處 謝謝 \"  \n",
      "\n",
      "label: pos \n",
      "\n",
      "tokens_tensor:\n",
      " tensor([  101,   722,  1184,  6554,  6525,  6857,  2157,   100,   100,  8188,\n",
      "         1366,  5388,  8485,   120, 10695,  8180,  8158,  8181,  8206,  1963,\n",
      "         5018,   671,  7514,   100,  6134,  3419,  7549,  4850,  2124,   699,\n",
      "         4192,  6127,  4886,  6956,  4638,  7015,  3332,  5226,  5998,  3791,\n",
      "         6211,   677,   889,  6303,   679,  5050,  7015,  4615,  4500,  2772,\n",
      "         1912,  4906,  2797,  6123,  1366,  5388,   852,  5018,   753,  7514,\n",
      "          100,   889,  6303,   100,  8188,  4638,   100,   100,  7344,  6362,\n",
      "         3126,  3362,  6963,  1920,  3176,   100,  8020,  5637,  4124,  1912,\n",
      "         4906,  1366,  5388,  3560,  3976,  8021,  2523,  7432,  2682,  1008,\n",
      "         3188,  4197,  6211,  3419,  3291,  7770,  4158,   862,   679,  1343,\n",
      "         1357,  2533,  6291,  6349,  6656,  7015,  3332,  5226,  5998,  2682,\n",
      "         6313,  3136,  3221,  1415,  1762,  6182,  4923,  4472,  1862,  2772,\n",
      "         1071,   800,  3313,  1154,  1139,  4638,  2825,  6123,  6211,  3419,\n",
      "          677,   793,  3300,   679,  1350,  1912,  4906,  2797,  6123,   722,\n",
      "         5993,  6342,  6342,   107,   102]) \n",
      "\n",
      "segment tensor:\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1])\n"
     ]
    }
   ],
   "source": [
    "# 隨便選一個樣本\n",
    "sample_idx = 1\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor,origin_text = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "\n",
    "print('token:\\n',tokens,'\\n')\n",
    "print('origin_text:\\n',origin_text,'\\n')\n",
    "print('label:',label_map[int(label_tensor.numpy())],'\\n')\n",
    "print('tokens_tensor:\\n',tokens_tensor,'\\n')\n",
    "print('segment tensor:\\n',segments_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 3636, 4031,  ...,    0,    0,    0],\n",
      "        [ 101, 3636, 4031,  ...,    0,    0,    0],\n",
      "        [ 101, 5080, 1606,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  722, 1184,  ...,    0,    0,    0],\n",
      "        [ 101, 2537, 4554,  ..., 3680, 3189,  102],\n",
      "        [ 101, 7015, 6362,  ..., 1751, 2157,  102]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\"\"\"\"\n",
    "create_mini_batch(samples)吃上面定義的mydataset\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "#collate_fn: 如何將多個樣本的資料連成一個batch丟進 model\n",
    "#截長補短後要限制attention只注意非pad 的部分\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 訓練集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad到該batch下最長的長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 batch size 個訓練樣本的 DataLoader\n",
    "# 利用 'collate_fn' 將 list of samples 合併成一個 mini-batch\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "\n",
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "print(tokens_tensors)\n",
    "print(segments_tensors)\n",
    "print(masks_tensors)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name      module\n",
      "--------------------\n",
      "bert      embeddings\n",
      "bert      encoder\n",
      "bert      pooler\n",
      "dropout    Dropout(p=0.1, inplace=False)\n",
      "classifier Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "name      module\n",
    "--------------------\"\"\")\n",
    "\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(\"{:10}{}\".format(name,n) )\n",
    "    else:\n",
    "        print(\"{:10} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "整個分類模型的參數量：102269186\n",
      "線性分類器的參數量：1538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "[epoch 1] loss: 0.8596, acc: 0.3571, val loss: 0.659854, val acc: 0.666667\n",
      "[epoch 2] loss: 0.6513, acc: 0.6429, val loss: 0.690108, val acc: 0.333333\n",
      "[epoch 3] loss: 0.5840, acc: 0.8571, val loss: 0.726005, val acc: 0.333333\n",
      "[epoch 4] loss: 0.5501, acc: 0.7857, val loss: 0.741894, val acc: 0.333333\n",
      "[epoch 5] loss: 0.4858, acc: 0.7857, val loss: 0.739206, val acc: 0.333333\n",
      "[epoch 6] loss: 0.4636, acc: 0.7857, val loss: 0.716468, val acc: 0.333333\n",
      "[epoch 7] loss: 0.3785, acc: 1.0000, val loss: 0.693205, val acc: 0.666667\n",
      "[epoch 8] loss: 0.3960, acc: 0.9286, val loss: 0.674147, val acc: 0.666667\n",
      "[epoch 9] loss: 0.3327, acc: 1.0000, val loss: 0.658977, val acc: 0.666667\n",
      "[epoch 10] loss: 0.2700, acc: 1.0000, val loss: 0.649096, val acc: 0.666667\n",
      "Done\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    correct = 0\n",
    "    #total = 0\n",
    "    train_loss , val_loss = 0.0 , 0.0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        n += 1\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        # outputs 的順序是 \"(loss), logits, (hidden_states), (attentions)\"\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #get prediction and calulate acc\n",
    "        logits = outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        train_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    #validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valloader:\n",
    "            m += 1\n",
    "            tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "            val_outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "            \n",
    "            logits = val_outputs[1]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            val_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "            val_loss += val_outputs[0].item()\n",
    "\n",
    "    print('[epoch %d] loss: %.4f, acc: %.4f, val loss: %4f, val acc: %4f' %\n",
    "          (epoch+1, train_loss/n, train_acc/n, val_loss/m,  val_acc/m  ))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  1]\n",
      " [ 0 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9411764705882353"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "true=[]\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in testloader:\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "        val_outputs = model(input_ids=tokens_tensors, \n",
    "                    token_type_ids=segments_tensors, \n",
    "                    attention_mask=masks_tensors, \n",
    "                    labels=labels)\n",
    "\n",
    "        logits = val_outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        true.extend(labels.cpu().tolist())\n",
    "        predictions.extend(pred.cpu().tolist())\n",
    "\n",
    "\n",
    "c = confusion_matrix(true, predictions)\n",
    "print(c)\n",
    "accuracy_score(predictions,true)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92         7\n",
      "           1       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.94        17\n",
      "   macro avg       0.95      0.93      0.94        17\n",
      "weighted avg       0.95      0.94      0.94        17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "classification acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted\n",
       "0           0\n",
       "1           1\n",
       "2           1\n",
       "3           0\n",
       "4           0\n",
       "5           1\n",
       "6           0\n",
       "7           0\n",
       "8           1\n",
       "9           1\n",
       "10          1\n",
       "11          1\n",
       "12          1\n",
       "13          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "predictions = get_predictions(model, trainloader)\n",
    "df = pd.DataFrame({\"predicted\": predictions.tolist()})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflowgpuenv\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>小弟朋友花了2000多元買9051 9051V 由於他找不到買主 所以他想要捐贈給也需求的第...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>之前購買這家MOTEX N 80口罩 cc/e5r6gx 如第一頁PDF表格顯示 它並無衛福...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>二個已經被爆出來的台商一定只是冰山一角 還有前面說的加拿大學生 都再再證明了 根本沒有自主隔...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>武漢肺炎潛伏期也具傳染性 陸官員：防控力度要再加強 聯合報 記者陳言喬 大陸國家衛生健康委員...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>在網路上有稍微查了一下 好像還是有快遞在運作 請問台灣要寄送的話 可以找哪一家快遞呢? 有沒...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>這位是補習班老師： facebook com/zi xu 7/posts/283579668...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>有武漢肺炎感染者封院 大甲李綜合醫院澄清 最近有LINE群組轉傳台中大甲李綜合醫院出現武漢肺...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>摘自八卦板文章 Re: [新聞] 武漢肺炎是人禍 中共病毒所員工爆料內幕 ptt cc/bb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>武漢肺炎（2019nCoV）疫情已漸成國際焦點 並影響台灣 為使國民重視防疫 並利於新聞情報...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>直播連結 be/4s5tFfw_eE 14:00開始 有回答了 他們座位離蠻遠的\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>中評社香港1月27日電蒙古政府宣布 為防範新型冠狀病毒傳播 即日起關閉連 接中國的邊境關口 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>從疫區返台台中男每天被關懷 李綜合醫院無武漢肺炎個案 台中市李姓男子日前到大陸武漢出差 除夕...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>醫護寧挨餓 不敢脫防護服吃飯 中國時報 藍孝威綜合報導 2020年1月26日 上午10:11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>簡單說 現在這情形了 還在扯什麼人性本善 尊重人權 人會自主管理的人 : 是有多天真 只說...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted                                               text\n",
       "0           0  小弟朋友花了2000多元買9051 9051V 由於他找不到買主 所以他想要捐贈給也需求的第...\n",
       "1           1  之前購買這家MOTEX N 80口罩 cc/e5r6gx 如第一頁PDF表格顯示 它並無衛福...\n",
       "2           1  二個已經被爆出來的台商一定只是冰山一角 還有前面說的加拿大學生 都再再證明了 根本沒有自主隔...\n",
       "3           0  武漢肺炎潛伏期也具傳染性 陸官員：防控力度要再加強 聯合報 記者陳言喬 大陸國家衛生健康委員...\n",
       "4           0  在網路上有稍微查了一下 好像還是有快遞在運作 請問台灣要寄送的話 可以找哪一家快遞呢? 有沒...\n",
       "5           1  這位是補習班老師： facebook com/zi xu 7/posts/283579668...\n",
       "6           0  有武漢肺炎感染者封院 大甲李綜合醫院澄清 最近有LINE群組轉傳台中大甲李綜合醫院出現武漢肺...\n",
       "7           0  摘自八卦板文章 Re: [新聞] 武漢肺炎是人禍 中共病毒所員工爆料內幕 ptt cc/bb...\n",
       "8           1  武漢肺炎（2019nCoV）疫情已漸成國際焦點 並影響台灣 為使國民重視防疫 並利於新聞情報...\n",
       "9           1         直播連結 be/4s5tFfw_eE 14:00開始 有回答了 他們座位離蠻遠的\" \n",
       "10          1  中評社香港1月27日電蒙古政府宣布 為防範新型冠狀病毒傳播 即日起關閉連 接中國的邊境關口 ...\n",
       "11          1  從疫區返台台中男每天被關懷 李綜合醫院無武漢肺炎個案 台中市李姓男子日前到大陸武漢出差 除夕...\n",
       "12          1  醫護寧挨餓 不敢脫防護服吃飯 中國時報 藍孝威綜合報導 2020年1月26日 上午10:11...\n",
       "13          1   簡單說 現在這情形了 還在扯什麼人性本善 尊重人權 人會自主管理的人 : 是有多天真 只說..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{0: 'neg', 1: 'pos'}\n",
    "df['text']= 'NaN'\n",
    "for i in range(len(df['predicted'])):\n",
    "    df['text'][i] = trainset[i][3]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
