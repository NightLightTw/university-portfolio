{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f0ea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Collecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Downloading tokenizers-0.11.5-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from transformers) (1.19.5)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: six in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.4.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611c47fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu110\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from transformers import BertTokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\" #中文\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd250313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2ec71e08fb4794b98ade37688b6504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c4521b0e2540cea410917b36b7f31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a134ad4fb9846e58a0e6ffb4d99e586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd102f0cc3d44fbad9e9451bb829ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size 21128\n",
      "token               index          \n",
      "-------------------------\n",
      "烘                    4167\n",
      "##串                 13763\n",
      "樣                    3564\n",
      "蟆                    6093\n",
      "mall                 9628\n",
      "##协                 14348\n",
      "##托                 15862\n",
      "溅                    3972\n",
      "##卯                 14369\n",
      "##簫                 18141\n"
     ]
    }
   ],
   "source": [
    "# get pre-train tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"dict size\", len(vocab))\n",
    "\n",
    "# see some token and index mapping\n",
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids): #隨便看幾個字\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719456fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 127\n",
      "valset size: 31\n",
      "testset size:  42\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset,random_split\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = TAG_RE.sub('', sen)\n",
    "    # Remove punctuations \n",
    "    sentence = re.sub('[，。,、；.？]', ' ', sentence)\n",
    "    # Removing URL\n",
    "    sentence = re.sub('[a-zA-z]+://[^\\s]*', ' ', sentence)\n",
    "    sentence = re.sub('/((?:https?\\:\\/\\/|www\\.)(?:[-a-z0-9]+\\.)*[-a-z0-9]+.*)/i', ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def readDATA(path, seg):\n",
    "    classes = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in classes:\n",
    "        files = os.listdir(os.path.join(path, seg, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([preprocess_text(review), 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([preprocess_text(review), 0])\n",
    "    return data\n",
    "\n",
    "label_map = {0: 'neg', 1: 'pos'}\n",
    "\n",
    "#create Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  \n",
    "        self.mode = mode\n",
    "        self.df = readDATA('dataset',mode) #its list [['text1',label],['text2',label],...]\n",
    "        self.len = len(self.df)\n",
    "        self.maxlen = 300 #限制文章長度(若你記憶體夠多也可以不限)\n",
    "        self.tokenizer = tokenizer  # we will use BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        origin_text = self.df[idx][0]\n",
    "        if self.mode == \"test\":\n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            #label_tensor = None #in our case, we have label\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        else:     \n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        \n",
    "        # 建立第一個句子的 BERT tokens\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a[:self.maxlen] + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        if text_b is not None:\n",
    "            tokens_b = self.tokenizer.tokenize(text_b)\n",
    "            word_pieces += tokens_b + [\"[SEP]\"]\n",
    "            len_b = len(word_pieces) - len_a\n",
    "               \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        if text_b is None:\n",
    "            segments_tensor = torch.tensor([1] * len_a,dtype=torch.long)\n",
    "        elif text_b is not None:\n",
    "            segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor, origin_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# initialize Dataset\n",
    "trainset = MyDataset(\"train\", tokenizer=tokenizer)\n",
    "testset = MyDataset(\"test\", tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#split val from trainset\n",
    "val_size = int(trainset.__len__()*0.2) #比對LSTM 切出1000筆當validation 0.04\n",
    "trainset, valset = random_split(trainset,[trainset.__len__()-val_size,val_size])\n",
    "print('trainset size:' ,trainset.__len__())\n",
    "print('valset size:',valset.__len__())\n",
    "print('testset size: ',testset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "967a59d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:\n",
      " ['[CLS]', '發', '稿', '單', '位', '：', '[UNK]', '發', '稿', '時', '間', '：', '2020', '年', '1', '月', '31', '日', '15', ':', '36', '撰', '稿', '者', '：', '曾', '金', '月', '原', '文', '連', '結', '：', 'tvbs', 'com', 'tw', '/', 'me', '##dical', '/', '322', '##53', '##3', '中', '國', '武', '漢', '肺', '炎', '疫', '情', '持', '續', '擴', '大', '國', '內', '已', '出', '現', '第', '二', '例', '本', '土', '病', '例', '醫', '師', '提', '醒', '若', '14', '天', '內', '有', '中', '國', '（', '不', '含', '港', '澳', '）', '旅', '遊', '史', '或', '接', '觸', '史', '且', '出', '現', '發', '燒', '肺', '炎', '或', '呼', '吸', '道', '症', '狀', '的', '民', '眾', '可', '直', '接', '到', '全', '台', '9', '家', '提', '供', '新', '型', '冠', '狀', '病', '毒', '篩', '檢', '的', '醫', '院', '就', '診', '約', '一', '天', '時', '間', '即', '可', '獲', '知', '結', '果', '包', '括', '台', '大', '醫', '院', '台', '北', '榮', '總', '三', '軍', '總', '醫', '院', '林', '口', '長', '庚', '中', '國', '醫', '藥', '大', '學', '附', '設', '醫', '院', '中', '山', '醫', '學', '大', '學', '附', '設', '醫', '院', '成', '大', '醫', '院', '高', '雄', '榮', '總', '和', '花', '蓮', '慈', '濟', '等', '9', '家', '醫', '院', '為', '現', '階', '段', '可', '提', '供', '新', '型', '冠', '狀', '病', '毒', '快', '速', '核', '酸', '檢', '驗', '的', '醫', '院', '皆', '為', '免', '費', '台', '灣', '感', '染', '症', '醫', '學', '會', '理', '事', '長', '台', '大', '兒', '科', '主', '任', '黃', '立', '民', '表', '示', '目', '前', '通', '報', '的', '標', '準', '主', '要', '看', '病', '史', '和', '旅', '遊', '史', '若', '14', '天', '內', '從', '湖', '北', '回', '來', '的', '民', '眾', '出', '現', '發', '燒', '咳', '嗽', '等', '上', '呼', '吸', '道', '症', '狀', '或', '是', '14', '天', '內', '到', '過', '中', '國', '大', '陸', '其', '他', '地', '[SEP]'] \n",
      "\n",
      "origin_text:\n",
      " 發稿單位：TVBS 發稿時間：2020年1月31日15:36 撰稿者：曾金月 原文連結： tvbs com tw/medical/322533 中國武漢肺炎疫情持續擴大 國內已出現第二例本土病例 醫師提醒 若14天內有中國（ 不含港澳）旅遊史或接觸史 且出現發燒 肺炎或呼吸道症狀的民眾 可直接到全台9家 提供新型冠狀病毒篩檢的醫院就診 約一天時間即可獲知結果 包括台大醫院 台北榮總 三軍總醫院 林口長庚 中國醫藥大學附設醫院 中山醫學大 學附設醫院 成大醫院 高雄榮總和花蓮慈濟等9家醫院 為現階段可提供新型冠狀病毒 快速核酸檢驗的醫院 皆為免費 台灣感染症醫學會理事長 台大兒科主任黃立民表示 目前通報的標準主要看病史和旅遊 史 若14天內從湖北回來的民眾 出現發燒 咳嗽等上呼吸道症狀 或是14天內到過中國 大陸其他地區的民眾 出現肺炎 發燒及上呼吸道症狀的狀況 這當然也包括他們的家人 黃立民指出 民眾到可快速檢驗新型冠狀病毒篩檢的醫院就診及進一步檢驗 實驗室檢驗 人目前加班待命 只要收到案件 大約24小時內可知道結果 \"  \n",
      "\n",
      "label: pos \n",
      "\n",
      "tokens_tensor:\n",
      " tensor([  101,  4634,  4943,  1606,   855,  8038,   100,  4634,  4943,  3229,\n",
      "         7279,  8038,  8439,  2399,   122,  3299,  8176,  3189,  8115,   131,\n",
      "         8216,  3066,  4943,  5442,  8038,  3295,  7032,  3299,  1333,  3152,\n",
      "         6865,  5178,  8038, 11657,  8134,  8351,   120,  8450, 13168,   120,\n",
      "        11965,  9310,  8152,   704,  1751,  3636,  4031,  5511,  4142,  4554,\n",
      "         2658,  2898,  5265,  3097,  1920,  1751,  1058,  2347,  1139,  4412,\n",
      "         5018,   753,   891,  3315,  1759,  4567,   891,  7015,  2374,  2990,\n",
      "         7008,  5735,  8122,  1921,  1058,  3300,   704,  1751,  8020,   679,\n",
      "         1419,  3949,  4078,  8021,  3180,  6879,  1380,  2772,  2970,  6240,\n",
      "         1380,   684,  1139,  4412,  4634,  4240,  5511,  4142,  2772,  1461,\n",
      "         1429,  6887,  4568,  4311,  4638,  3696,  4707,  1377,  4684,  2970,\n",
      "         1168,  1059,  1378,   130,  2157,  2990,   897,  3173,  1798,  1094,\n",
      "         4311,  4567,  3681,  5072,  3596,  4638,  7015,  7368,  2218,  6262,\n",
      "         5147,   671,  1921,  3229,  7279,  1315,  1377,  4363,  4761,  5178,\n",
      "         3362,  1259,  2886,  1378,  1920,  7015,  7368,  1378,  1266,  3532,\n",
      "         5244,   676,  6725,  5244,  7015,  7368,  3360,  1366,  7269,  2423,\n",
      "          704,  1751,  7015,  5973,  1920,  2119,  7353,  6257,  7015,  7368,\n",
      "          704,  2255,  7015,  2119,  1920,  2119,  7353,  6257,  7015,  7368,\n",
      "         2768,  1920,  7015,  7368,  7770,  7413,  3532,  5244,  1469,  5709,\n",
      "         5909,  2705,  4089,  5023,   130,  2157,  7015,  7368,  4158,  4412,\n",
      "         7389,  3667,  1377,  2990,   897,  3173,  1798,  1094,  4311,  4567,\n",
      "         3681,  2571,  6862,  3417,  7000,  3596,  7710,  4638,  7015,  7368,\n",
      "         4639,  4158,  1048,  6527,  1378,  4124,  2697,  3381,  4568,  7015,\n",
      "         2119,  3298,  4415,   752,  7269,  1378,  1920,  1051,  4906,   712,\n",
      "          818,  7941,  4989,  3696,  6134,  4850,  4680,  1184,  6858,  1841,\n",
      "         4638,  3560,  3976,   712,  6206,  4692,  4567,  1380,  1469,  3180,\n",
      "         6879,  1380,  5735,  8122,  1921,  1058,  2537,  3959,  1266,  1726,\n",
      "          889,  4638,  3696,  4707,  1139,  4412,  4634,  4240,  1495,  1644,\n",
      "         5023,   677,  1461,  1429,  6887,  4568,  4311,  2772,  3221,  8122,\n",
      "         1921,  1058,  1168,  6882,   704,  1751,  1920,  7380,  1071,   800,\n",
      "         1765,   102]) \n",
      "\n",
      "segment tensor:\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# 隨便選一個樣本\n",
    "sample_idx = 1\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor,origin_text = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "\n",
    "print('token:\\n',tokens,'\\n')\n",
    "print('origin_text:\\n',origin_text,'\\n')\n",
    "print('label:',label_map[int(label_tensor.numpy())],'\\n')\n",
    "print('tokens_tensor:\\n',tokens_tensor,'\\n')\n",
    "print('segment tensor:\\n',segments_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba666722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  138, 3315,  ...,  694, 6868,  102],\n",
      "        [ 101,  722, 1184,  ...,    0,    0,    0],\n",
      "        [ 101, 1963, 3362,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  791, 1921,  ...,    0,    0,    0],\n",
      "        [ 101,  107, 1963,  ...,    0,    0,    0],\n",
      "        [ 101, 4634, 4943,  ..., 7519, 1086,  102]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\"\"\"\"\n",
    "create_mini_batch(samples)吃上面定義的mydataset\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "#collate_fn: 如何將多個樣本的資料連成一個batch丟進 model\n",
    "#截長補短後要限制attention只注意非pad 的部分\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 訓練集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad到該batch下最長的長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 batch size 個訓練樣本的 DataLoader\n",
    "# 利用 'collate_fn' 將 list of samples 合併成一個 mini-batch\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "\n",
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "print(tokens_tensors)\n",
    "print(segments_tensors)\n",
    "print(masks_tensors)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b75628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caca4799ad3d4231b404b4dec5838b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name      module\n",
      "--------------------\n",
      "bert      embeddings\n",
      "bert      encoder\n",
      "bert      pooler\n",
      "dropout    Dropout(p=0.1, inplace=False)\n",
      "classifier Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "name      module\n",
    "--------------------\"\"\")\n",
    "\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(\"{:10}{}\".format(name,n) )\n",
    "    else:\n",
    "        print(\"{:10} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55bd5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "整個分類模型的參數量：102269186\n",
      "線性分類器的參數量：1538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd137b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "[epoch 1] loss: 0.6948, acc: 0.5599, val loss: 0.730901, val acc: 0.418750\n",
      "[epoch 2] loss: 0.6422, acc: 0.6385, val loss: 0.786105, val acc: 0.452083\n",
      "[epoch 3] loss: 0.5463, acc: 0.7333, val loss: 0.835380, val acc: 0.452083\n",
      "[epoch 4] loss: 0.4651, acc: 0.7948, val loss: 0.847953, val acc: 0.452083\n",
      "[epoch 5] loss: 0.4087, acc: 0.8729, val loss: 0.958061, val acc: 0.354167\n",
      "[epoch 6] loss: 0.2991, acc: 0.9609, val loss: 1.045828, val acc: 0.352083\n",
      "[epoch 7] loss: 0.2402, acc: 0.9531, val loss: 1.154833, val acc: 0.416667\n",
      "[epoch 8] loss: 0.1520, acc: 0.9844, val loss: 1.258087, val acc: 0.450000\n",
      "[epoch 9] loss: 0.1050, acc: 1.0000, val loss: 1.479439, val acc: 0.450000\n",
      "[epoch 10] loss: 0.0767, acc: 1.0000, val loss: 1.523832, val acc: 0.416667\n",
      "Done\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    correct = 0\n",
    "    #total = 0\n",
    "    train_loss , val_loss = 0.0 , 0.0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        n += 1\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        # outputs 的順序是 \"(loss), logits, (hidden_states), (attentions)\"\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #get prediction and calulate acc\n",
    "        logits = outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        train_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    #validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valloader:\n",
    "            m += 1\n",
    "            tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "            val_outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "            \n",
    "            logits = val_outputs[1]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            val_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "            val_loss += val_outputs[0].item()\n",
    "\n",
    "    print('[epoch %d] loss: %.4f, acc: %.4f, val loss: %4f, val acc: %4f' %\n",
    "          (epoch+1, train_loss/n, train_acc/n, val_loss/m,  val_acc/m  ))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41566e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  5]\n",
      " [11 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6190476190476191"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "true=[]\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in testloader:\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "        val_outputs = model(input_ids=tokens_tensors, \n",
    "                    token_type_ids=segments_tensors, \n",
    "                    attention_mask=masks_tensors, \n",
    "                    labels=labels)\n",
    "\n",
    "        logits = val_outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        true.extend(labels.cpu().tolist())\n",
    "        predictions.extend(pred.cpu().tolist())\n",
    "\n",
    "\n",
    "c = confusion_matrix(true, predictions)\n",
    "print(c)\n",
    "accuracy_score(predictions,true)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c63f6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.76      0.67        21\n",
      "           1       0.67      0.48      0.56        21\n",
      "\n",
      "    accuracy                           0.62        42\n",
      "   macro avg       0.63      0.62      0.61        42\n",
      "weighted avg       0.63      0.62      0.61        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f370a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "classification acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fc4b50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted\n",
       "0            1\n",
       "1            1\n",
       "2            0\n",
       "3            0\n",
       "4            1\n",
       "..         ...\n",
       "122          1\n",
       "123          1\n",
       "124          0\n",
       "125          1\n",
       "126          1\n",
       "\n",
       "[127 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "predictions = get_predictions(model, trainloader)\n",
    "df = pd.DataFrame({\"predicted\": predictions.tolist()})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54880377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zizhong\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>雖然我本身在醫院工作 不用擔心採購問題 但是這專業口罩規格資訊還是給大家參考 快樂小藥師這資...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>發稿單位：TVBS 發稿時間：2020年1月31日15:36 撰稿者：曾金月 原文連結： t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>我看今天疾管屬目前的安排接納剩餘待在武漢4百多人方式感覺到隱憂 從之前台商自我 管理就哪個樣...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>媒體來源:聯合新聞網 沒原料口罩趕工 有人力也沒轍 20200204 00:22聯合報 記者...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>發稿單位：聯合報 發稿時間：20200130 14:15 撰 稿 者：編譯馮克芸 原文連結：...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>拜託你們這些人 : 本人我是中醫版前版主 : 背景是西醫藥的專業人員 : 我不否認西醫症狀治...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>已經有人分析到 這次武漢肺炎病毒可怕的地方在於會使醫療資源耗盡 ptt cc/bbs/Gos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>我跟enuj同想法 就刷身分證字號的條碼就好 建置一個口罩的資料庫 大概就是以身分證字號為個...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "      <td>中國人口14億是什麼概念? 歐盟人口5億 美國人口3億 日本人口1億 全中國的人都要戴口罩 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>就目前得到的資訊： 1 潛伏期可長達一週 2 不一定會發燒 3 傳染途經很廣範（目前已知至少...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted                                               text\n",
       "0            1  雖然我本身在醫院工作 不用擔心採購問題 但是這專業口罩規格資訊還是給大家參考 快樂小藥師這資...\n",
       "1            1  發稿單位：TVBS 發稿時間：2020年1月31日15:36 撰稿者：曾金月 原文連結： t...\n",
       "2            0  我看今天疾管屬目前的安排接納剩餘待在武漢4百多人方式感覺到隱憂 從之前台商自我 管理就哪個樣...\n",
       "3            0  媒體來源:聯合新聞網 沒原料口罩趕工 有人力也沒轍 20200204 00:22聯合報 記者...\n",
       "4            1  發稿單位：聯合報 發稿時間：20200130 14:15 撰 稿 者：編譯馮克芸 原文連結：...\n",
       "..         ...                                                ...\n",
       "122          1  拜託你們這些人 : 本人我是中醫版前版主 : 背景是西醫藥的專業人員 : 我不否認西醫症狀治...\n",
       "123          1  已經有人分析到 這次武漢肺炎病毒可怕的地方在於會使醫療資源耗盡 ptt cc/bbs/Gos...\n",
       "124          0  我跟enuj同想法 就刷身分證字號的條碼就好 建置一個口罩的資料庫 大概就是以身分證字號為個...\n",
       "125          1  中國人口14億是什麼概念? 歐盟人口5億 美國人口3億 日本人口1億 全中國的人都要戴口罩 ...\n",
       "126          1  就目前得到的資訊： 1 潛伏期可長達一週 2 不一定會發燒 3 傳染途經很廣範（目前已知至少...\n",
       "\n",
       "[127 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{0: 'neg', 1: 'pos'}\n",
    "df['text']= 'NaN'\n",
    "for i in range(len(df['predicted'])):\n",
    "    df['text'][i] = trainset[i][3]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1dd2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
